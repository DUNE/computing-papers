%%% CHEP 2019 protoDUNE-SP DQM evolution paper

%%%\documentclass[option]{webofc}
%%% "twocolumn" for typesetting an article in two columns format (default one column)


\documentclass{webofc}
\usepackage[varg]{txfonts}   % Web of Conferences font

%
% Put here some packages required or/and some personnal commands
%

\usepackage{xspace}
\usepackage{tabularx}

\newcommand{\pd}{protoDUNE\xspace}
\newcommand{\filesize}{8\,GB\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%
\title{Evolution of the Data Quality Monitoring and Prompt Processing System in the protoDUNE-SP experiment}


\author{
\firstname{Maxim} 
\lastname{Potekhin}\inst{1}\fnsep\thanks{\email{potekhin@bnl.gov}}, \it{on behalf of the DUNE Collaboration}
}

\institute{Brookhaven National Laboratory, Upton, NY11973, USA}


\abstract{
The DUNE Collaboration currently operates
an experimental program based at CERN which includes a beam test and an extended
cosmic ray run of two large-scale prototypes of the massive Liquid Argon Time
Projection Chamber for the DUNE Far Detector. The volume of data collected by the single-phase prototype
(protoDUNE-SP) amounts to $\sim$3PB and the sustained rate
of data sent to mass storage is O(100)\,MB/s.  Data Quality Monitoring
was implemented by directing a fraction of the data stream
to the protoDUNE Prompt Processing System (p3s) which is optimized
for continuous low-latency calculation of the vital detector metrics and various graphics
including event displays. It served a crucial role throughout the life cycle of the
experiment. We present our experience in leveraging the CERN computing
environment and operating the system over an extended period of time, while
adapting to evolving requirements and computing platform.
}
%
\maketitle
%
\section{Introduction}
\label{sec:intro}

The goal of the \pd-SP experiment  \cite{spanu} is a detailed study of a large-scale prototype
of the massive 10kt single-phase Liquid Argon Time Projection Chambers (LArTPC) to be used
in the DUNE experiment  \cite{cdrVol1, cdrVol4}. The experiment took test beam data in
2018 and cosmic ray data throughout 2019. The nature of the experiment requires a versatile
Data Quality Monitoring (DQM) system capable of performing complex calculations on
a few minutes time scale as well as preserving the results (inluding various types of graphics
and tabulated data) in an easily accessible form and over an extended period of time.
In order to support this capability a \textit{protoDUNE prompt processing system} (p3s)
was designed, deployed and operated over of a period of time of $\sim$3 years \cite{eps}. Its design proved flexible
enough to quickly adapt to different hardware platforms and computational payloads.

\section{System Design}
\label{sec:outline}

A predefined (and configurable) fraction of the data written to a local buffer
by the  \pd-SP Data Acquisition (DAQ) system is  transferred to a dedicated space
in the  CERN EOS disk storage  \cite{castoreos,eos_role} by an instance of the ``Fermi-FTS''
data handling system \cite{fts}. Arrival of new data is detected by an automated process
polling the corresponsing directory in EOS and triggers creation of jobs records in the
p3s database, forming a queue. The p3s brokerage module then matches these jobs records
to available pilot jobs (running asynchronously on the worker nodes within the designated
resource) and provides the pilot with information necessary to actuate the ``payload job'' (e.g.\,as via a URL
pointing to a script and a set of parameters). Monitoring information about execution
of the payloads is presented to the user in the p3s Web UI.
Job outputs are automatically catalogued in the database by a dedicated Web service
and are made available to the users via a Web interface through a set of selectors and menus.
The following is a summary  of the \pd-SP prompt processing/DQM system design features\cite{chep18}:
\begin{itemize}

\item A high degree of automation and a data-driven workflow
%: creation of various computational payloads is triggered by arrival of new data

\item Complete decoupling of the DQM and the DAQ systems, which allows
for stable and reliable operation of the latter while being able to make frequent updates to the
DQM code and adding features and new types of software modules (``payloads'') whenever
necessary

\item A simple pilot-based framework inspired by PanDA and Dirac \cite{panda,dirac}
which minimizes the latency of job submission and provides a layer of abstraction
of the computing resources; a Web application coordinates the operation of ``pilot jobs''
which in turn manage execution of the DQM jobs defined by the user (the ``payload jobs'')

\item A JSON schema for job description including job templates

\item Separation of the workload management functionality  of p3s  from
indexing and navigation of the content created by the DQM  jobs (a separate ``content service''),
which provides a clean interface and optimal end-user experience

\item Industry-standard tools and frameworks: Django\cite{django} applications
run with the Apache HTTP server and the PostgreSQL RDBMS back-end

\item Self-describing DQM data which allows automatic and consistent generation of Web pages
by the content service for each type of DQM applications without changes to the server code.
This is achieved by the requirement that DQM jobs produce metadata (a set of JSON files)
which completely describes their output dataset and the logic of its navigation

\end{itemize}
\noindent
A conceptual diagram of the prompt processing and DQM system components
is presented in figure~\ref{fig:dqm-diagram}.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth,clip]{figures/dqm-p3s-diagram.png}
\caption{The protoDUNE-SP prompt processing and DQM systems}
\label{fig:dqm-diagram}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Evolution}

\subsection{The Hardware Platform}
\label{sec:hardware}

The p3s pilot-based framework provides substantial flexibility in accessing and
managing the available computing resource. Generation of pilot jobs is external
to the core system itself and thus can be easily adapted to a particular resource.
One basic requirement is that the pilot jobs running on the worker nodes have
HTTP access to the p3s server. The first (test) instance of p3s deployed
on an \textit{ad hoc} colleciton of local machines. The parallel remote shell
(pdsh) was used to manage population of pilot jobs running on the worker nodes.
The next deployment was on a hardware cluster provided by the CERN
Neutrino Platform organization \cite{platform} consisting of a few dozen
worker nodes configured as a HTCondor cluster, and the pilots were managed
by scripts utilizing the HTCondor batch system tools. Web and database
services were deployed on two nodes which were part of the same cluster.
Subsequently a decision was made to leverage the capabilities of the CERN
Central Services which included the following:

\begin{itemize}

\item hosting the p3s server and DQM content server on the CERN OpenStack
Virtual Machines

\item utilization  the central batch facility (``lxbatch'') as the compute element

\item relying on the EOS distributed storage to stage input and output data
including serving the DQM content through the Web service

\end{itemize}

\noindent The migration included adapting existing scripts to the new HTCondor
environment and using the Kerberized ``acrontab'' facility to orchestrate their
periodic execution. The input data was staged from EOS storage to the worker
nodes' local disk space prior to execution using the \textit{xrdcp} utility.

For the p3s and DQM servers deployed in OpenStack we made
use of the snapshot feature to simplify recovery and to scale the services as
needed, for example by transparently increasing the number of cores in a host
VM while keeping the software environment intact.

Our experience of long-term operation within the CERN environment
is positive. We note exceptional stability of the OpenStack VMs.
Intermittent latency fluctuations of file transfers to worker nodes
were attributed to periods of extremely high load on the EOS cluster
and accounted for. The p3s dashboard was updated with
a table of metrics for the batch system, storage space and
file system performance to help operators assess the state of the
system and take corrective meastures.

\subsection{The Software}
\label{sec:software}

Throught its life cycle the experiment required an evolving set of algorithms to
be included in its DQM software, sometimes with frequent software updates.
At various points in time the following algorithms were active:

\begin{itemize}

\item 2D event display (based on raw data)

\item The TPC monitoring application with an extensive set of histograms at various
levels of channel aggregation, FFT charts etc (with $\sim$1000 histograms produced
in each application run)

\item Health monitor for the Front End Electronics motherboards (e.g. for detection of
signal dropouts)

\item Prompt reconstruction of cosmic ray track candidates with metrics such as
track candidate count and hit count in various parts of the detector

\item Liquid Argon purity estimations based on charge values for track candidates;
time series were stored in the database and rendered graphically in the Web UI as shown in
a screenshot in figure \ref{fig:purity}

\item Signal-to-noise ratio monitoring (time series stored in the database and rendered graphically in the Web UI)

\item Data preparation for the 3D event display implemented on a separate server

\end{itemize}

\noindent System settings allowed to configure the number of jobs allocated to each job type,
for optimal resource management. Adding new types of jobs was facilitated by two factors:

\begin{itemize}

\item Reliance on existing job templates which required only minor adjustments

\item Self-describing DQM output data simplifying integration with the content service \cite{chep18}

\end{itemize}

\noindent Software updates and its provisioning to the worker nodes were simplified by making use of
the CVMFS distribution model.

\ 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth,clip]{figures/purity2019.png}
\caption{Screenshot: time series of the electron lifetime in the Liquid Argon of the protoDUNE-SP TPC, based on
DQM data for cosmic ray track candidates}
\label{fig:purity}
\end{figure}

\subsection{Conclusions}

The \pd-SP Data Quality Monitoring system has been one one of the crucial components of the experiment.
It demonstrated a high degree of flexibility and was quickly adapted to the changing hardware platforms and
the evolving DQM software mainly due to the following design features:
\begin{itemize}
\item Pilot framework used in the workload management part of the system
\item Standardized templates for DQM job description
\item Transparent and reliable software updates via CVMFS
\item Self-describing DQM data which simplified integration of new DQM job types into the sytem
\end{itemize}

\noindent It is foreseen that the system will be used in the projected Run 2 of \pd-SP and may potentially
serve as a prototype for a DQM system in the DUNE experiment.



\begin{thebibliography}{99}

\bibitem{spanu}
M. Spanu
\emph{The status and results from ProtoDUNE Single Phase}
IOP Conf. Series: Journal of Physics: Conf. Series \textbf{1312} (2019) 012003

\bibitem{cdrVol1}
R. Acciarri et al.
\emph{Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) Conceptual Design Report Volume 1: The LBNF and DUNE Projects}.\\ ~e-Print: arXiv:\textbf{1601.05471}


\bibitem{cdrVol4}
R. Acciarri et al.
\emph{Long-Baseline Neutrino Facility (LBNF) and Deep Underground Neutrino Experiment (DUNE) Conceptual Design Report, Volume 4: The DUNE Detectors at LBNF}.\\~e-Print: arXiv:\textbf{1601.02984}

\bibitem{eps} M.Potekhin et al. \emph{The protoDUNE-SP experiment and its prompt
processing system}. Proceedings of Science (EPS-HEP2017) 513


\bibitem{castoreos}
L. Mascetti et al. \emph{Disk storage at CERN.~J. Phys.: Conf. Series.} Vol.\textbf{664}. IOP Publishing, 2015,
doi:10.1088/1742-6596/664/4/042035

\bibitem{eos_role}
S. Fuess et al. \emph{Design of the protoDUNE raw data management
infrastructure.~J. Phys.: Conf. Series.} Vol.\textbf{898}. IOP Publishing, 2017,
doi:10.1088/1742-6596/898/6/062036

\bibitem{fts}
A. Norman \emph{The Fermilab File Transfer System}.~e-Print: FNAL CD-DocDB-5412

\bibitem{chep18} M. Potekhin et al. \emph{The Prompt Processing System
and Data Quality Monitoring in the protoDUNE-SP Experiment}
EPJ Web of Conferences \textbf{214}, 01026 (2019)

\bibitem{panda}
T. Maeno et al. \emph{Overview of ATLAS PanDA Workload Management.~J. Phys.: Conf. Series.} Vol.\textbf{331}. IOP Publishing, 2011,
doi:10.1088/1742-6596/331/7/072024


\bibitem{dirac}
A. Casajus et al.  \emph{DIRAC Pilot Framework and the DIRAC
Workload Management System.~J. Phys.: Conf. Series.} Vol.\textbf{219}. IOP Publishing, 2010,
doi:10.1088/1742-6596/219/6/062049


\bibitem{django}
N. George \emph{Mastering Django: Core. The Complete Guide to Django 1.8 LTS}~ GNW Independent Publishing, ISBN: 099461683X

\bibitem{platform} F. Pietropaolo \emph{Review of Liquid-Argon Detectors Development at the CERN
Neutrino Platform}
IOP Conf. Series:  Journal of Physics: Conf. Series \textbf{888} (2017) 012038


\end{thebibliography}




\end{document}

% end of file template.tex

<div id='footer'><table width='100%'><tr><td class='right'><a href='http://fusioninventory.org/'><span class='copyright'>FusionInventory 9.1+1.0 | copyleft <img src='/glpi/plugins/fusioninventory/pics/copyleft.png'/>  2010-2016 by FusionInventory Team</span></a></td></tr></table></div>